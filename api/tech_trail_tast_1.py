# -*- coding: utf-8 -*-
"""Tech-Trail Tast 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BqxOxcOTnJHDbXaeTyAF8BOfWwTrDFOf
"""

# -------------------------------
# Mount Drive & Install Libraries
# -------------------------------
from google.colab import drive
drive.mount('/content/drive')

!pip install pandas numpy seaborn matplotlib scikit-learn joblib

# -------------------------------
# Step 1: Import Libraries
# -------------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib
import os
from google.colab import files

# -------------------------------
# Step 2: Upload datasets
# -------------------------------
print("Upload booking.csv")
uploaded_booking = files.upload()
booking_file = list(uploaded_booking.keys())[0]
booking = pd.read_csv(booking_file, parse_dates=['booking_date','appointment_date','check_in_time','check_out_time'])
print("\nBooking Missing Values:\n", booking.isnull().sum())

print("\nUpload task.csv")
uploaded_task = files.upload()
task_file = list(uploaded_task.keys())[0]
task = pd.read_csv(task_file)
print("Task Dataset:")
print(task.head())

print("\nUpload staff.csv")
uploaded_staff = files.upload()
staff_file = list(uploaded_staff.keys())[0]
staff = pd.read_csv(staff_file, parse_dates=['date'])
print("Staff Dataset:")
print(staff.head())

print("\nDatasets uploaded successfully!")

# -----------------------------------------------
# Step 3: Quick EDA / Visualization per dataset
# -----------------------------------------------
print("Booking Data Shape:", booking.shape)
display(booking.head())
print("\nBooking Missing Values:\n", booking.isnull().sum())

# Visualize booking: number of bookings per task
sns.countplot(x='task_id', data=booking)
plt.title("Number of bookings per task")
plt.show()

# Task dataset
print("\nTask Data Shape:", task.shape)
display(task.head())
print("\nTask Missing Values:\n", task.isnull().sum())

# Staff dataset
print("\nStaff Data Shape:", staff.shape)
display(staff.head())
print("\nStaff Missing Values:\n", staff.isnull().sum())

# Visualize staff: total_task_time_minutes distribution
sns.histplot(staff['total_task_time_minutes'], kde=True)
plt.title("Distribution of total task time per section/day")
plt.show()

# -------------------------------
# Step 4: Merge datasets
# -------------------------------
# Booking + Task
df = booking.merge(task[['task_id','section_id']], on='task_id', how='left')

# Booking + Staff (via date and section_id)
df = df.merge(staff, left_on=['appointment_date','section_id'], right_on=['date','section_id'], how='left')

# -------------------------------
# Step 5: Compute target
# -------------------------------
df['completion_time_minutes'] = (df['check_out_time'] - df['check_in_time']).dt.total_seconds() / 60
df = df.dropna(subset=['completion_time_minutes'])
df = df[df['completion_time_minutes'] > 0]

# -------------------------------
# Step 6: Feature Engineering
# -------------------------------
df['appointment_hour'] = df['appointment_time'].str.split(':').str[0].astype(int)
df['appointment_weekday'] = df['appointment_date'].dt.weekday
df['is_weekend'] = df['appointment_weekday'].apply(lambda x: 1 if x>=5 else 0)
df['month'] = df['appointment_date'].dt.month

# Staff-related features (available from staff.csv)
df['staff_load_ratio'] = df['num_documents'] / (df['employees_on_duty']+1)  # avoid div by zero

# Encode categorical features
le_task = LabelEncoder()
df['task_id_encoded'] = le_task.fit_transform(df['task_id'].astype(str))

le_section = LabelEncoder()
df['section_id_encoded'] = le_section.fit_transform(df['section_id'].astype(str))

# --------------------------------------
# Step 7: Select features & standardize
# --------------------------------------
features = [
    'appointment_hour','appointment_weekday','is_weekend','month',
    'staff_load_ratio','employees_on_duty','task_id_encoded','section_id_encoded'
]

X = df[features]
y = df['completion_time_minutes']

# Standardize numeric features
num_features = ['appointment_hour','appointment_weekday','month','staff_load_ratio','employees_on_duty']
scaler = StandardScaler()
X[num_features] = scaler.fit_transform(X[num_features])

# -------------------------------
# Step 8: Split train/test
# -------------------------------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --------------------------------------------------
# Step 9: Train Model (XGBoost for better accuracy)
# --------------------------------------------------
xgb = XGBRegressor(
    n_estimators=300,
    learning_rate=0.1,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

xgb.fit(X_train, y_train)

# -------------------------------
# Step 10: Predictions & Evaluation (Updated)
# -------------------------------
y_pred = xgb.predict(X_test)

# Standard regression metrics
MAE  = mean_absolute_error(y_test, y_pred)
RMSE = np.sqrt(mean_squared_error(y_test, y_pred))
R2   = r2_score(y_test, y_pred)

# Mean Absolute Percentage Error for percentage-style evaluation
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100

# Print a clean summary
print("\n=== Model Training Completed Successfully ===")
print(f"MAE  : {MAE:.2f} minutes")
print(f"RMSE : {RMSE:.2f} minutes")
print(f"R2   : {R2:.2f}")
print(f"MAPE : {mape:.2f}%")

# -------------------------------
# Step 11: Feature Importance
# -------------------------------
feat_importances = pd.Series(xgb.feature_importances_, index=features)
plt.figure(figsize=(10,6))
sns.barplot(x=feat_importances, y=feat_importances.index)
plt.title("Feature Importances")
plt.show()

# -------------------------------
# Step 12: Save model & scaler
# -------------------------------
model_dir = '/content/drive/MyDrive/Task1/models'
os.makedirs(model_dir, exist_ok=True)

joblib.dump(xgb, os.path.join(model_dir,'xgb_service_completion_model.pkl'))
joblib.dump(scaler, os.path.join(model_dir,'scaler.pkl'))
joblib.dump(le_task, os.path.join(model_dir,'task_label_encoder.pkl'))
joblib.dump(le_section, os.path.join(model_dir,'section_label_encoder.pkl'))

# -------------------------------
# Step 13: Save metrics
# -------------------------------
metrics_file = os.path.join(model_dir,'model_metrics.txt')
with open(metrics_file,'w') as f:
    f.write(f"MAE: {MAE:.2f}\n")
    f.write(f"RMSE: {RMSE:.2f}\n")
    f.write(f"R2: {R2:.2f}\n")

print("\nModel, scaler, and metrics saved successfully in Drive!")

# -------------------------------
# Step 13: Prediction Function (Final Integration)
# -------------------------------
def predict_completion_time(date, time_str, task_id):
    """
    Predicts service completion time in minutes.
    Inputs: date (YYYY-MM-DD), time (HH:MM), task_id
    """
    date = pd.to_datetime(date)
    hour = int(time_str.split(':')[0])
    weekday = date.weekday()
    is_weekend = 1 if weekday >= 5 else 0
    month = date.month

    # Encode task_id
    task_id_encoded = le_task.transform([str(task_id)])[0]

    # Since section info comes from task mapping:
    section_id = task.loc[task['task_id']==task_id, 'section_id'].values[0]
    section_id_encoded = le_section.transform([str(section_id)])[0]

    # Approximate staff features (if available, otherwise set defaults)
    staff_row = staff[(staff['date']==date) & (staff['section_id']==section_id)]
    if not staff_row.empty:
        employees_on_duty = staff_row['employees_on_duty'].values[0]
        staff_load_ratio = staff_row['num_documents'].values[0] / (employees_on_duty+1)
    else:
        employees_on_duty = 5  # default
        staff_load_ratio = 1.0

    # Build feature vector
    features_dict = {
        'appointment_hour': hour,
        'appointment_weekday': weekday,
        'is_weekend': is_weekend,
        'month': month,
        'staff_load_ratio': staff_load_ratio,
        'employees_on_duty': employees_on_duty,
        'task_id_encoded': task_id_encoded,
        'section_id_encoded': section_id_encoded
    }

    input_df = pd.DataFrame([features_dict])

    # Scale numeric features
    input_df[num_features] = scaler.transform(input_df[num_features])

    # Predict
    predicted_minutes = xgb.predict(input_df)[0]
    return round(predicted_minutes,2)

plt.figure(figsize=(8,6))
sns.heatmap(df[features + ['completion_time_minutes']].corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlation Heatmap")
plt.show()